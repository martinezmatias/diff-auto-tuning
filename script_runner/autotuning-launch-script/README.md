
# Appendix of DAT (Diff-Auto-Tuning) 

## At Java runner parameters

```
@Option(names = "-out", required = true)
@Option(names = "-path", required = true)
@Option(names = "-subset", required = true)
@Option(names = "-begin")
@Option(names = "-stop", defaultValue = "10000000")
@Option(names = "-astmodel", required = false, defaultValue = "GTSPOON")
@Option(names = "-parallel", defaultValue = "true")
```


## Experiments:

There are two kinds of scripts.
1) The scripts that read the data (encoded in CSV) that DAT generates and generates summary of data (also in CSV) and/or plots.
In particular, DAT creates, for each diff file, two files:
a) one with the results of the execution of different diff algorithms with different configurations (example `nr_1_id_1_000056e_SingleAction_GTSPOON.csv`), 
b) some metrics (size of trees under comparision, execution time)  (example `metaInfo_nr_571_id_21_0c944315_Jukebox_GTSPOON.csv`) 

2) The scripts that consume data generated by the scripts previously mentioned.

## Scripts that consume and summarize raw data generated by DAT.

###  Main Basic Script

The script `ResultsAnalyzeDiffConfiguration.py` reads the csv generated by DAT and creates a single CSV file with the performance of each configuration on each file.
 
* It creates `distance_per_diff.csv` a matrix where the column are the configurations, the rows are the diff file and the cells are the distances


## Some setups

`Script_RQ0_Setup.py` has some functionalities than can be executed before the others.
1) To merge two datasets.
2) To get the total numbers of diff.



### Research Question 1 Optimization of each algoritm

First, we need to create the already mentioned file `distance_per_diff_[X].csv` 
using the test `test_ComputeFitnessFastPerAlgorithm` from the `ScriptResearchQuestion1_GridSearch_OptimizationPerAlgo.py`

#### GridSearch

The script `EngineGridSearchKfoldValidation.py`  method `computeGridSearchKFold`
computes the grid search k-times.
The input is the `distance_per_diff_[X].csv` previously computed (where X is the name of the algorithm) 
It also compares with the default configuration.

To execute it, run `ScriptResearchQuestion1_GridSearch_OptimizationPerAlgo.py`

There are two methods to execute (test cases).
First, `test_ComputeBestKFoldComplete` which computes the k-fold validation (calling `computeGridSearchKFold`) taking as input a `distance_per_diff_[X].csv` per algorithm 
That generates as results different files but the most important is one csv with the performance of each configuration per validation: 
`summary_performance_performance_[DATASET]_4_K_0_Gumtree.csv`
Moreover, it present a file `summary_merge_[dataset]_Gumtree_performanceTestingDefaultOnTraining.csv` which presents on each line K the best performance value of the k validation.

Second, we need to generated the Tables and plots.
The method `test_ComparisonBest_PlotRQ1` plots the distribution of performance and consumes both files.
The method ``_test_ComparisonBest_PlotRQ1`` calls `computeBestAndDefaultByFoldFiles` which computes the numbers for the table  based on the `summary_performance_performance_[DATASET]_4_K_0_Gumtree.csv` files


#### HyperOpt
First,  install dependencies. Execute: ` pip install hyperopt pingouin sklearn`.

`EngineHyperOptDAT.py` executes HyperOpt framework, in particular the TPE method.
The input is the `distance_per_diff_[X].csv` previously computed (where X is the name of the algorithm).

There are two methods to execute (test cases) from `ScriptRQ1_HyperOpt_optimization.py`.
First, to run HyperOptDat, we call `test_CompteHyperOpt_single_by_algo` which generates a csv (e.g. `hyper_op_merge_gtJDT_5_CDJDT_4_XyMatcher_evals_50_f_0.005.csv`) with 
the best configurations and their performance per k validation.
Here, we consider a particular dataset (fraction) and a particular number of evaluation.
Second, the test `testAnalyzeHyperopResults` reads those files and prints the results for the paper.


### Research Question 2: Trees comparison.


The tests from `/src/rowDataConsumers/ScriptRQ2_CompareASTmetamodel.py` executes the comparison by calling script `src/rowDataConsumers/EngineCompareASTMetadata.py`
For this analysis, only is necesary the row data (in particular the )


### Research Question 3: Performance of different metamodels.


### Research Question 4: Best Algorithm

First, we need to compute the `distance_per_diff.csv` 
(with the fitness values considering all the algorithms, not per algorithm as in RQ1)
For that we execute from `ScriptsRQ4_BestAlgorithm.py` the test `_test_ComputeFitnessFast`

Then, todo.

## Research Question 5: Cost

The script `ScriptRQ5_Cost_HyperOpt.py` contains two methods.
First, `_test_CompteHyperOpt_range` executes HyperOpts with different a) number of diff in the dataset, b) number of evaluations.
Then, `_test_Study_Evolution_HyperOpt` analyzes the results from the previous execution. (TO FINISH)

Two new runner to execute the experimentt on the Grid:
`ScriptRQ5_Runner_Cost_GridSearchKCrossValidationAllAlgo.py `
and `ScriptRQ5_Runner_Cost_TPE.py`(replace the mentioned first one in `ScriptRQ5_Cost_HyperOpt.py`)

####


#### Comparison of distributions

`src/processedDataConsumers/ResultsCompareDistribution.py ` does a test for comparing the distribution of two configurations.
The input is the `distance_per_diff.csv` previously computed.

### Deprecated Scripts 
####   Complete results (deprecated)

From `ResultsAnalyzeBestComplete.py`  the method `computeBestConfigurations` summarize the results.
It's an optional script. 
This is a heavy scripts, not necessary to run, that can take several hours to execute.

* It creates a `best_configurations_summary.csv` file with the metrics of each configuration, including the number of time the config is the best.
Each  row is a configuration.
* It creates a `matrix_overlap_configs.csv` which contains the overlap of best between two configurations.
Each row and column is a configuration. Cells are the overlap.
* It creates `single_configs_[METRIC]PerConfiguration.csv`, where [METRIC] is a metric between time, size and height.
It stores measures (avg, median) for each metric.
* It creatres `single_parameter_[METRIC]PerConfiguration.csv` similar to the previous one, but each focus on a single parameter.
Each row summarizes all the configuration gaving a particular pair of hyperparameter and value.
* It creates `distance_per_diff.csv` a matrix where the column are the configurations, the rows are the diff file and the cells are the distances

#### Relation Time and Size (deprecated)

Scripts `ResultsAnalyzeRelationTimeSize.py` computes the relation between time and size.
It produces two outputs: a) a csv that contains the regression line computed from the points(size, time) b) the plots (disable)

#### Execution Times (deprecated)

Script `ResultsAnalyzeTimes.py` generates violing plots the results according to the execution times grouped by diff algorithms and  Megadiff group id.


# Script that consume and analyze generated summmary files (no row data)

## Position of default configuration (deprecated)

`ResultsReadCheckPositionDefault.py` returns the index of each default configuration.
This script is invoked by the cross-validation script. It's not necessary to call it.

## Relation of times between best and not best (deprecated)

`ResultsReadGeneratedFilesTimeComputer.py` analyzes the relation between the time of diff when a configuration is the best and when it's not the best.
It creates a plot `plot_time_best_X` and print the correlation between best and not best times found by a diff algorithm X.
The axe X are the configuration, sorted by best from left to write.

# Configuration

The file `DiffAlgorithmMetadata.py` contains the hyperparameters of each algorithm.


## Merge datasets:

The script `src/commons/DatasetMerger.py` merges two different executions (e.g., two folders produced by DAT).
In particular, as I executed GumTree and ChangeDistiller separatelly, this script merges the raw results.

## Last results

### GTSpoon

Total diff 32647 total config 66926352
1443.085s

Total diff 24850 total config 50942500

